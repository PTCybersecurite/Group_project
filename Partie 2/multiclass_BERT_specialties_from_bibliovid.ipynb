{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass_BERT_specialties_from_bibliovid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyFfOuo3bPJ3"
      },
      "source": [
        "# Multiclass specialties classifier for Bibliovid using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6JKbtqVfQcS"
      },
      "source": [
        "Mounting our gdrive folder and importing libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJiaOWqLncIO",
        "outputId": "ff046608-7f45-4652-9a3d-ad4c9761bad6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuMlXT80GAMK"
      },
      "source": [
        "# Importing the libraries needed\n",
        "!pip install -q transformers\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from sklearn import metrics\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBit7M9O3LTG"
      },
      "source": [
        "## Loading the preproccessed dataset (preproccessing in data_prep)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnzKC6-7-DFs"
      },
      "source": [
        "DATA_FOLDER = '/content/drive/MyDrive/PSTALN/data/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SCxYBQenRAc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "5099e17c-d0a6-4a16-917d-9db39df29e70"
      },
      "source": [
        "df = pd.read_pickle(DATA_FOLDER+'mc_clean_df_bibliovid_pretreated.pkl')\r\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>slug</th>\n",
              "      <th>title</th>\n",
              "      <th>has_other_authors</th>\n",
              "      <th>impact_factor</th>\n",
              "      <th>goals_plain</th>\n",
              "      <th>verbose_date</th>\n",
              "      <th>authors</th>\n",
              "      <th>document_link</th>\n",
              "      <th>specialties</th>\n",
              "      <th>category</th>\n",
              "      <th>journal</th>\n",
              "      <th>link</th>\n",
              "      <th>results</th>\n",
              "      <th>synthesis</th>\n",
              "      <th>strength_of_evidence_details</th>\n",
              "      <th>goals</th>\n",
              "      <th>methods</th>\n",
              "      <th>pubmed_id</th>\n",
              "      <th>doi</th>\n",
              "      <th>abstract</th>\n",
              "      <th>topics</th>\n",
              "      <th>author_list</th>\n",
              "      <th>publication_date</th>\n",
              "      <th>vect_specs</th>\n",
              "      <th>cat_text</th>\n",
              "      <th>len</th>\n",
              "      <th>input_ids</th>\n",
              "      <th>attention_mask</th>\n",
              "      <th>token_type_ids</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>ENCODE_CAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>769</td>\n",
              "      <td>body-mass-index-and-risk-for-intubation-or-dea...</td>\n",
              "      <td>Body Mass Index and Risk for Intubation or Dea...</td>\n",
              "      <td>True</td>\n",
              "      <td>{'id': 3, 'name': 'Intermédiaire', 'posts_coun...</td>\n",
              "      <td>- Déterminer si l'obésité est associée à l'int...</td>\n",
              "      <td>31.07.2020</td>\n",
              "      <td>Anderson MR</td>\n",
              "      <td>https://www.acpjournals.org/doi/10.7326/M20-3214</td>\n",
              "      <td>[{'id': 4, 'name': 'Anesthésie-Réanimation'}, ...</td>\n",
              "      <td>{'id': 6, 'name': 'Pronostique', 'icon': 'icon...</td>\n",
              "      <td>{'id': 41, 'name': 'Ann Intern Med'}</td>\n",
              "      <td>https://www.acpjournals.org/doi/10.7326/M20-3214</td>\n",
              "      <td>*Description de l'échantillon: 2112 patients c...</td>\n",
              "      <td>- Environ 2-3% des patients atteints de la COV...</td>\n",
              "      <td>-cohorte pronostique- puissance de l'étude sup...</td>\n",
              "      <td>- Déterminer si l'obésité est associée à l'int...</td>\n",
              "      <td>Cohorte rétrospective portant sur 2466 patient...</td>\n",
              "      <td>32726151</td>\n",
              "      <td>10.7326/M20-3214</td>\n",
              "      <td>Obesity is a risk factor for pneumonia and acu...</td>\n",
              "      <td>[Pronostique, Anesthésie-Réanimation, Infectio...</td>\n",
              "      <td>[{'id': 714, 'name': 'Anderson MR'}]</td>\n",
              "      <td>2020-07-31</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>Body Mass Index and Risk for Intubation or Dea...</td>\n",
              "      <td>259</td>\n",
              "      <td>[101, 2303, 3742, 5950, 3891, 20014, 19761, 35...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>Body Mass Index and Risk for Intubation or Dea...</td>\n",
              "      <td>Anesthésie-Réanimation</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>742</td>\n",
              "      <td>an-mrna-vaccine-against-sars-cov-2-preliminary...</td>\n",
              "      <td>An mRNA Vaccine against SARS-CoV-2 - Prelimina...</td>\n",
              "      <td>True</td>\n",
              "      <td>{'id': 3, 'name': 'Intermédiaire', 'posts_coun...</td>\n",
              "      <td>Développement du vaccin accéléré mRNA-1273 Mod...</td>\n",
              "      <td>15.07.2020</td>\n",
              "      <td>Jackson LA</td>\n",
              "      <td>https://www.nejm.org/doi/10.1056/NEJMoa2022483</td>\n",
              "      <td>[{'id': 22, 'name': 'Immunité'}, {'id': 5, 'na...</td>\n",
              "      <td>{'id': 4, 'name': 'Thérapeutique', 'icon': 'ic...</td>\n",
              "      <td>{'id': 22, 'name': 'NEJM'}</td>\n",
              "      <td>https://www.nejm.org/doi/10.1056/NEJMoa2022483</td>\n",
              "      <td>Après la première vaccination, les réponses an...</td>\n",
              "      <td>Le vaccin mRNA-1273 est plutôt bien toléré. De...</td>\n",
              "      <td>Les résultats du rapport ne sont que prélimina...</td>\n",
              "      <td>Développement du vaccin accéléré mRNA-1273 Mod...</td>\n",
              "      <td>Essai de vaccination de Phase 1, ouvert inclua...</td>\n",
              "      <td>32663912</td>\n",
              "      <td>10.1056/NEJMoa2022483</td>\n",
              "      <td>The severe acute respiratory syndrome coronavi...</td>\n",
              "      <td>[Thérapeutique, Immunité, Virologie, Infectiol...</td>\n",
              "      <td>[{'id': 690, 'name': 'Jackson LA'}]</td>\n",
              "      <td>2020-07-15</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>An mRNA Vaccine against SARS-CoV-2 - Prelimina...</td>\n",
              "      <td>269</td>\n",
              "      <td>[101, 28848, 17404, 18906, 9363, 2615, 2475, 8...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>An mRNA Vaccine against SARS-CoV-2 - Prelimina...</td>\n",
              "      <td>Immunité</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>739</td>\n",
              "      <td>pathophysiology-transmission-diagnosis-and-tre...</td>\n",
              "      <td>Pathophysiology, Transmission, Diagnosis, and ...</td>\n",
              "      <td>True</td>\n",
              "      <td>{'id': 2, 'name': 'Faible', 'posts_count': 505...</td>\n",
              "      <td>Etat des lieux bibliographique des connaissanc...</td>\n",
              "      <td>14.07.2020</td>\n",
              "      <td>Joost Wiersinga W</td>\n",
              "      <td>https://jamanetwork.com/journals/jama/fullarti...</td>\n",
              "      <td>[{'id': 7, 'name': 'Transversale'}, {'id': 12,...</td>\n",
              "      <td>{'id': 2, 'name': 'Autres', 'icon': 'icon-other'}</td>\n",
              "      <td>{'id': 183, 'name': 'JAMA Network Open'}</td>\n",
              "      <td>https://jamanetwork.com/journals/jama/fullarti...</td>\n",
              "      <td>La transmission du SARS-CoV-2 est plus favorab...</td>\n",
              "      <td>Actualisation générale des connaissances (rech...</td>\n",
              "      <td>Revue orientée d'études pré-sélectionnées par ...</td>\n",
              "      <td>Etat des lieux bibliographique des connaissanc...</td>\n",
              "      <td>Bases de données indexées (générale et récente...</td>\n",
              "      <td>32648899</td>\n",
              "      <td>10.1001/jama.2020.12839</td>\n",
              "      <td>The coronavirus disease 2019 (COVID-19) pandem...</td>\n",
              "      <td>[Autres, Transversale, Infectiologie]</td>\n",
              "      <td>[{'id': 687, 'name': 'Joost Wiersinga W'}]</td>\n",
              "      <td>2020-07-14</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>Pathophysiology, Transmission, Diagnosis, and ...</td>\n",
              "      <td>436</td>\n",
              "      <td>[101, 4130, 7361, 10536, 20763, 6483, 6726, 11...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>Pathophysiology, Transmission, Diagnosis, and ...</td>\n",
              "      <td>Transversale</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>735</td>\n",
              "      <td>introductions-and-early-spread-of-sars-cov-2-i...</td>\n",
              "      <td>Introductions and Early Spread of SARS-CoV-2 i...</td>\n",
              "      <td>True</td>\n",
              "      <td>{'id': 4, 'name': 'Indéterminé', 'posts_count'...</td>\n",
              "      <td>Etudier comment l'épidémie de SARS-Cov-2 a com...</td>\n",
              "      <td>10.07.2020</td>\n",
              "      <td>Gambaro F</td>\n",
              "      <td>https://www.eurosurveillance.org/content/10.28...</td>\n",
              "      <td>[{'id': 7, 'name': 'Transversale'}, {'id': 5, ...</td>\n",
              "      <td>{'id': 5, 'name': 'Epidémiologique', 'icon': '...</td>\n",
              "      <td>{'id': 46, 'name': 'Eurosurveillance'}</td>\n",
              "      <td>https://www.eurosurveillance.org/content/10.28...</td>\n",
              "      <td>Le virus a été introduit plusieurs fois dans l...</td>\n",
              "      <td>Le virus SARS-Cov-2 a été introduit plusieurs ...</td>\n",
              "      <td>Les données de cette étude semblent disponible...</td>\n",
              "      <td>Etudier comment l'épidémie de SARS-Cov-2 a com...</td>\n",
              "      <td>Données. 97 séquences de SARS-Cov-2 recueillie...</td>\n",
              "      <td>32643599\\n32289214\\n32070465\\n32109013\\n321797...</td>\n",
              "      <td>10.2807/1560-7917.ES.2020.25.26.2001200</td>\n",
              "      <td>Following SARS-CoV-2 emergence in China, a spe...</td>\n",
              "      <td>[Epidémiologique, Transversale, Virologie]</td>\n",
              "      <td>[{'id': 628, 'name': 'Gambaro F'}]</td>\n",
              "      <td>2020-07-10</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>Introductions and Early Spread of SARS-CoV-2 i...</td>\n",
              "      <td>88</td>\n",
              "      <td>[101, 25795, 2220, 3659, 18906, 9363, 2615, 24...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>Introductions and Early Spread of SARS-CoV-2 i...</td>\n",
              "      <td>Transversale</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>724</td>\n",
              "      <td>how-to-safely-reopen-colleges-and-universities...</td>\n",
              "      <td>How to Safely Reopen Colleges and Universities...</td>\n",
              "      <td>True</td>\n",
              "      <td>{'id': 2, 'name': 'Faible', 'posts_count': 505...</td>\n",
              "      <td>Décrire l'expérience des universités de Taïwan...</td>\n",
              "      <td>03.07.2020</td>\n",
              "      <td>Cheng SY</td>\n",
              "      <td>https://www.acpjournals.org/doi/10.7326/M20-2927</td>\n",
              "      <td>[{'id': 21, 'name': 'Confinement/Déconfinement...</td>\n",
              "      <td>{'id': 5, 'name': 'Epidémiologique', 'icon': '...</td>\n",
              "      <td>{'id': 41, 'name': 'Ann Intern Med'}</td>\n",
              "      <td>https://www.acpjournals.org/doi/10.7326/M20-2927</td>\n",
              "      <td>A Taïwan jusqu'au 18 juin 2020 seuls 7 cas con...</td>\n",
              "      <td>Les universités de Taïwan ont adopté des mesur...</td>\n",
              "      <td>- retour d'expérience d'une seule université T...</td>\n",
              "      <td>Décrire l'expérience des universités de Taïwan...</td>\n",
              "      <td>Retour d'expérience de Taïwan sur la gestion d...</td>\n",
              "      <td>32614638</td>\n",
              "      <td>10.7326/M20-2927</td>\n",
              "      <td>Reopening colleges and universities during the...</td>\n",
              "      <td>[Epidémiologique, Confinement/Déconfinement, I...</td>\n",
              "      <td>[{'id': 675, 'name': 'Cheng SY'}]</td>\n",
              "      <td>2020-07-03</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
              "      <td>How to Safely Reopen Colleges and Universities...</td>\n",
              "      <td>239</td>\n",
              "      <td>[101, 9689, 2128, 26915, 6667, 5534, 2522, 172...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>How to Safely Reopen Colleges and Universities...</td>\n",
              "      <td>Confinement/Déconfinement</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id  ... ENCODE_CAT\n",
              "0  769  ...          2\n",
              "1  742  ...          1\n",
              "2  739  ...         14\n",
              "3  735  ...         14\n",
              "4  724  ...         10\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbngdj2nqsbG"
      },
      "source": [
        "## Loading the training and testing set in pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrBr2YesGdO_"
      },
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 11\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vX7kzaAHu39"
      },
      "source": [
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.TITLE[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcwq13c0NE9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30a0905-bbd1-4455-8aa3-84ff9b1cbe73"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (371, 33)\n",
            "TRAIN Dataset: (297, 33)\n",
            "TEST Dataset: (74, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1BgA1CkQSYa"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSSfdS84bPKI"
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 17)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lGIe-5PjbPKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ef87c6-f0cd-43da-d524-d90009020e03"
      },
      "source": [
        "model = DistillBERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistillBERTClass(\n",
              "  (l1): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKzS5tzFbPKK"
      },
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdrPuy7LbPKL"
      },
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gQqyux83LTM"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjsOQT1UbPKL"
      },
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0),total=len(training_loader),position=0,leave=True):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "         # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qnBsVfUbPKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5866d2f-0fb8-4cf8-bd21-0386f56d0dc4"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/75 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 75/75 [00:06<00:00, 11.99it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:06, 12.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 0: 34.343434343434346\n",
            "Training Loss Epoch: 2.4163862625757853\n",
            "Training Accuracy Epoch: 34.343434343434346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 12.03it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:06, 12.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 1: 38.72053872053872\n",
            "Training Loss Epoch: 2.2062454080581664\n",
            "Training Accuracy Epoch: 38.72053872053872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 11.95it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:06, 11.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 2: 38.72053872053872\n",
            "Training Loss Epoch: 2.1822320302327474\n",
            "Training Accuracy Epoch: 38.72053872053872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 12.05it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:05, 12.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 3: 38.72053872053872\n",
            "Training Loss Epoch: 2.1739904991785686\n",
            "Training Accuracy Epoch: 38.72053872053872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 12.07it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:05, 12.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 4: 38.72053872053872\n",
            "Training Loss Epoch: 2.161035122871399\n",
            "Training Accuracy Epoch: 38.72053872053872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 12.09it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:06, 11.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 5: 38.72053872053872\n",
            "Training Loss Epoch: 2.1356934650739032\n",
            "Training Accuracy Epoch: 38.72053872053872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 11.99it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:05, 12.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 6: 38.72053872053872\n",
            "Training Loss Epoch: 2.1144136349360148\n",
            "Training Accuracy Epoch: 38.72053872053872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 11.98it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:05, 12.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 7: 38.72053872053872\n",
            "Training Loss Epoch: 2.0320830782254538\n",
            "Training Accuracy Epoch: 38.72053872053872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 11.95it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:06, 11.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 8: 42.42424242424242\n",
            "Training Loss Epoch: 1.8928504876295726\n",
            "Training Accuracy Epoch: 42.42424242424242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 11.99it/s]\n",
            "  3%|▎         | 2/75 [00:00<00:06, 11.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 9: 53.872053872053876\n",
            "Training Loss Epoch: 1.6527996174494426\n",
            "Training Accuracy Epoch: 53.872053872053876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:06<00:00, 11.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 10: 60.94276094276094\n",
            "Training Loss Epoch: 1.4093180731932322\n",
            "Training Accuracy Epoch: 60.94276094276094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vhhn_aJzGGZ"
      },
      "source": [
        "Looking at how well the model performs on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5UCKFUkMW2b"
      },
      "source": [
        "def validation():\r\n",
        "    model.eval()\r\n",
        "    fin_targets=[]\r\n",
        "    fin_outputs=[]\r\n",
        "    with torch.no_grad():\r\n",
        "        for _, data in tqdm(enumerate(testing_loader),total=len(testing_loader),position=0,leave=True):\r\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\r\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\r\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\r\n",
        "            outputs = model(ids, mask)\r\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\r\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\r\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjS_TZwQMXec"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "outputs, targets = validation()\r\n",
        "outputs = [np.argmax(output) for output in outputs]\r\n",
        "accuracy = metrics.accuracy_score(targets, outputs)\r\n",
        "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\r\n",
        "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\r\n",
        "print()\r\n",
        "print(f\"Accuracy Score = {accuracy}\")\r\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\r\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVg9F-15E20a"
      },
      "source": [
        "# Results\r\n",
        "\r\n",
        "Nb epochs|Accuracy|F1 (micro)|F1 (macro)\r\n",
        "---|--- |---|---\r\n",
        "11|47.2%|47.2%|21.5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C_yk1FKbPKO"
      },
      "source": [
        "# Saving the files for re-use\n",
        "\n",
        "output_model_file = '/content/drive/MyDrive/PSTALN (1)/pytorch_distillbert_med.bin'\n",
        "output_vocab_file = '/content/drive/MyDrive/PSTALN (1)/'\n",
        "\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqVBHc43CvtG"
      },
      "source": [
        "torch.load('/content/drive/MyDrive/PSTALN (1)/pytorch_distillbert_med.bin')\r\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('/content/drive/MyDrive/PSTALN (1)/vocab_distillbert_med.bin')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}